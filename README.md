# ğŸ“š Costruire un Chatbot con RAG e Strumenti Open Source
**Esempio mostrato durante il Webinar "Costruire un Chatbot con RAG e Strumenti Open Source"**

âš ï¸ **Nota Bene:**  
Questo progetto Ã¨ stato realizzato a scopo dimostrativo per il webinar e **non Ã¨ pensato per un utilizzo in produzione**.  

---

## ğŸ“Œ Descrizione del Progetto
Il progetto mostra come costruire un **chatbot RAG (Retrieval Augmented Generation)** utilizzando strumenti **open source**.  
Il sistema sfrutta **[Ollama](https://ollama.ai)** come provider LLM per eseguire il modello **in locale**, integrato con **LangChain** per gestire il flusso RAG.  
I dati vengono indicizzati tramite **ChromaDB** come **vector store**, mentre l'interazione con il chatbot avviene tramite:  
- **API REST (Flask)** per le richieste al modello  
- **Interfaccia utente (Streamlit)** per una chat semplice e interattiva  

---

## ğŸ”§ Struttura del Progetto

```
â”œâ”€â”€ Chain/
â”‚   â”œâ”€â”€ inference_endpoint.py    # Definisce la chain RAG e l'endpoint di inferenza
â”‚   â””â”€â”€ __init__.py              # Descrizione del package Chain
â”‚
â”œâ”€â”€ Config/
â”‚   â”œâ”€â”€ ollama_config.py         # Gestione installazione e verifica modelli Ollama
â”‚   â”œâ”€â”€ vector_database_config.py# Creazione e gestione del ChromaDB
â”‚   â”œâ”€â”€ config_chatbot.py        # Pipeline di configurazione completa
â”‚   â”œâ”€â”€ colors.py                # Utility per messaggi colorati in console
â”‚   â””â”€â”€ __init__.py              # Descrizione del package Config
â”‚
â”œâ”€â”€ Data/                        # Directory contenente i file sorgente (.txt, pdf o md) da indicizzare
â”‚
â”œâ”€â”€ chatbot_api_endpoint.py      # API Flask per interagire con il modello
â”œâ”€â”€ app.py                       # Interfaccia Streamlit del chatbot
â”œâ”€â”€ requirements.txt             # Dipendenze del progetto
```

---

## âš™ï¸ Requisiti

- **Python 3.10+**
- [Ollama](https://ollama.ai) (installato e aggiunto al PATH)
- Modelli Ollama (embedding e chat model)

---

## ğŸ“¥ Installazione

### 1ï¸âƒ£ Clonare il progetto
```bash
git clone https://github.com/DevDocentiELIS/AIWebinarChatBot.git
cd AIWebinarChatBot
```

### 2ï¸âƒ£ Creare un ambiente virtuale e installare le dipendenze
```bash
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)

pip install -r requirements.txt
```

### 3ï¸âƒ£ Installare **Ollama**
Scaricare e installare [Ollama](https://ollama.ai).  
Verificare che sia disponibile:
```bash
ollama --version
```

### 4ï¸âƒ£ Avviare i modelli necessari
Il progetto utilizza i modelli:
- **Embedding:** `nomic-embed-text:latest`
- **Chat LLM:** `mistral:latest` (puÃ² essere sostituito con `llama3.2:latest` o altri modelli compatibili con le risorse hardware a disposizione)

[Lista e caratteristiche dei modelli disponibili](https://ollama.com/library)

Ãˆ possibile scaricarli manualmente:
```bash
ollama pull nomic-embed-text:latest
ollama pull mistral:latest
```

âš ï¸ **In alternativa**, la configurazione automatica si occuperÃ  di scaricare i modelli se non presenti.

---

## â–¶ï¸ Avvio del Progetto

### 1ï¸âƒ£ Avviare le API Flask
```bash
python chatbot_api_endpoint.py
```
Le API saranno disponibili su **http://localhost:5000/ask**

### 2ï¸âƒ£ Avviare lâ€™interfaccia Streamlit
In un **nuovo terminale**:
```bash
streamlit run app.py
```

L'interfaccia sarÃ  disponibile su **http://localhost:8501**

---

## ğŸ§  Come Funziona

1. **Caricamento dei dati locali** (`Data/` - puoi inserire un certo numero di file .pdf, .txt o .md) â†’ i file vengono suddivisi in chunk e convertiti in **embedding** tramite Ollama.
2. **Creazione del vector store** â†’ i chunk indicizzati vengono salvati in **ChromaDB**.
3. **Configurazione LLM (ChatOllama)** â†’ viene avviato un modello Ollama per rispondere alle domande.
4. **Esecuzione RAG Pipeline** â†’ la query dellâ€™utente viene arricchita con il contesto recuperato dal vector store.
5. **Interfaccia** â†’ lâ€™utente interagisce tramite API Flask o tramite lâ€™interfaccia Streamlit.

- **Il file: config_chatbot.py** contiene le impostazioni relative al sistema, puoi modificarle in questa parte del file:

```
__CONFIG_PARAMS = {
    "embedding_model": "nomic-embed-text:latest",
    "chat_model": "mistral:latest",
    "data_source_path": os.path.join(__BASE_DIR, "Data"),
    "data_files_extension": "txt",
    "text_chunk_size": 1000,
    "text_chunks_overlap": 0,
    "vector_store_dir": "chatbot_vector_storage"
}
```


---

## ğŸ”— Endpoints Disponibili

### **POST /ask**
Richiesta:
```json
{
  "question": "Scrivi qui la tua domanda"
}
```

Risposta (streaming):
```
Risposta generata dal modello...
```

---

## ğŸ“¦ Dipendenze Principali
ğŸ“„ `requirements.txt` include:
- **langchain**, **langchain-ollama**, **langchain-chroma**
- **flask**, **flask-cors**, **streamlit**
- **torch**, **requests**, **unstructured**

---

## âš ï¸ Avvertenze
ğŸš¨ **Questo progetto Ã¨ solo a scopo dimostrativo.**
- Non Ã¨ ottimizzato per produzione.
- Non include gestione avanzata degli errori o sicurezza.
- Ãˆ pensato per mostrare lâ€™integrazione RAG + Ollama + LangChain in modo semplice.
